{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxppqeL2vCxkq+sRS6T8+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchuang0710/Finetuning-on-ART/blob/main/Finetuning_on_ART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjk1YocKAkxK",
        "outputId": "ddd83cb5-985d-4760-c0f4-b2435f90d00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TinyLlama-1.1B-Chat-v0.3' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "!pip install fsspec==2023.9.2\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OnuSj-GAq7H",
        "outputId": "1f1acda5-0d40-488e-89fe-e1a8f5cb217f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2023.9.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: fsspec==2023.9.2 in /usr/local/lib/python3.11/dist-packages (2023.9.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2023.9.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iybdQRdY_a7v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "import json\n",
        "import gzip\n",
        "import random\n",
        "import torch\n",
        "from transformers import (\n",
        "    LlamaTokenizerFast, LlamaForCausalLM,\n",
        "    Trainer, TrainingArguments, BitsAndBytesConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(file_path):\n",
        "  with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n",
        "    return json.load(fp)\n",
        "\n",
        "def load_jsonl(file_path):\n",
        "  data = []\n",
        "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      data.append(json.loads(line))\n",
        "  return data\n",
        "\n",
        "def dump_json_gz(data, file_path):\n",
        "  with gzip.open(file_path, \"wt\", encoding=\"utf-8\") as fp:\n",
        "    json.dump(data, fp, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "o5wpFr6e_nGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = load_jsonl(\"ART.jsonl\")  # <<=== 你的全資料\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(full_dataset)\n",
        "\n",
        "train = full_dataset[:int(len(full_dataset)*0.8)]\n",
        "dev = full_dataset[int(len(full_dataset)*0.8):int(len(full_dataset)*0.9)]\n",
        "test = full_dataset[int(len(full_dataset)*0.9):]\n",
        "\n",
        "with open(\"train.json\", \"w\", encoding=\"utf-8\") as f: json.dump(train, f, ensure_ascii=False, indent=2)\n",
        "with open(\"dev.json\", \"w\", encoding=\"utf-8\") as f: json.dump(dev, f, ensure_ascii=False, indent=2)\n",
        "with open(\"test.json\", \"w\", encoding=\"utf-8\") as f: json.dump(test, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "d-pl3Y4a_qsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"TinyLlama-1.1B-Chat-v0.3\"\n",
        "tokenizer = LlamaTokenizerFast.from_pretrained(model_id)\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"Please determine whether these command using which MITRE technique. Just reply most similar technique ID, not to explain.\n",
        "\n",
        "Command:\n",
        "{}\n",
        "Answer:\"\"\"\n",
        "\n",
        "def iter_dataset(file_path):\n",
        "  data = load_json(file_path)\n",
        "  for sample in data:\n",
        "    user_content, assistant_content = \"\", \"\"\n",
        "    for msg in sample[\"messages\"]:\n",
        "      if msg[\"role\"] == \"user\":\n",
        "        user_content = msg[\"content\"]\n",
        "      elif msg[\"role\"] == \"assistant\":\n",
        "        assistant_content = msg[\"content\"]\n",
        "    yield user_content, assistant_content\n",
        "\n",
        "def build_prompt(user_content):\n",
        "    return PROMPT_TEMPLATE.format(user_content)"
      ],
      "metadata": {
        "id": "WGFTwpc4_v1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(file_path, tokenizer):\n",
        "  ds_tokens = []\n",
        "  for user_content, assistant_content in iter_dataset(file_path):\n",
        "    prompt = build_prompt(user_content)\n",
        "    full_text = f\"{prompt} {assistant_content}\"\n",
        "    tokens = tokenizer.encode(full_text) + [tokenizer.eos_token_id]\n",
        "    ds_tokens.append(tokens)\n",
        "\n",
        "  maxlen = max(len(seq) for seq in ds_tokens)\n",
        "  dataset = []\n",
        "  for tokens in ds_tokens:\n",
        "    padded = tokens + [tokenizer.eos_token_id] * (maxlen - len(tokens))\n",
        "    dataset.append({\"input_ids\": padded, \"labels\": padded})\n",
        "\n",
        "  return dataset, maxlen\n",
        "\n",
        "train_tokens, maxlen_train = tokenize_dataset(\"train.json\", tokenizer)\n",
        "dev_tokens, maxlen_dev = tokenize_dataset(\"dev.json\", tokenizer)\n",
        "\n",
        "dump_json_gz(train_tokens, \"train.tokens.json.gz\")\n",
        "dump_json_gz(dev_tokens, \"dev.tokens.json.gz\")"
      ],
      "metadata": {
        "id": "AqCb-aMr_4Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\n",
        "  \"train\": \"train.tokens.json.gz\",\n",
        "  \"dev\": \"dev.tokens.json.gz\"\n",
        "}\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=data_files)"
      ],
      "metadata": {
        "id": "Ww_un4hD_47t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_quant_type=\"fp4\",       # 精度較低但省顯存\n",
        "  bnb_4bit_use_double_quant=False, # 關閉雙重量化，省顯存\n",
        "  bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "  model_id,\n",
        "  quantization_config=bnb_config,\n",
        "  device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "1S4bbvnI_8EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "  r=8,\n",
        "  lora_alpha=16,\n",
        "  lora_dropout=0.1,\n",
        "  bias=\"none\",\n",
        "  task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "fLqPyW5PG9QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"Models/TinyLlama-1B-MITRE\"\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=output_dir,\n",
        "  per_device_train_batch_size=1,     # batch=1，最小化顯存\n",
        "  per_device_eval_batch_size=1,\n",
        "  gradient_accumulation_steps=8,     # 累積8次才更新，等效 batch size=8\n",
        "  eval_strategy=\"steps\",\n",
        "  save_strategy=\"steps\",\n",
        "  eval_steps=50,\n",
        "  save_steps=50,\n",
        "  save_total_limit=2,\n",
        "  num_train_epochs=3,\n",
        "  fp16=True,                         # 15GB GPU 建議用 fp16\n",
        "  bf16=False,\n",
        "  logging_steps=10,\n",
        "  run_name=\"TinyLlama-MITRE-QLoRA-15GB\"\n",
        ")"
      ],
      "metadata": {
        "id": "AEPggvISAKVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=training_args,\n",
        "  train_dataset=dataset[\"train\"],\n",
        "  eval_dataset=dataset[\"dev\"],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "J5QomyWOAMy6",
        "outputId": "2d5ffdf2-c8a4-425a-85ee-29de62fa82b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjchuang0710\u001b[0m (\u001b[33mjchuang0710-national-yang-ming-chiao-tung-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250719_144730-82gmerg0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jchuang0710-national-yang-ming-chiao-tung-university/huggingface/runs/82gmerg0' target=\"_blank\">TinyLlama-MITRE-QLoRA-15GB</a></strong> to <a href='https://wandb.ai/jchuang0710-national-yang-ming-chiao-tung-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jchuang0710-national-yang-ming-chiao-tung-university/huggingface' target=\"_blank\">https://wandb.ai/jchuang0710-national-yang-ming-chiao-tung-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jchuang0710-national-yang-ming-chiao-tung-university/huggingface/runs/82gmerg0' target=\"_blank\">https://wandb.ai/jchuang0710-national-yang-ming-chiao-tung-university/huggingface/runs/82gmerg0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [486/486 1:07:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.636700</td>\n",
              "      <td>3.695029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.342800</td>\n",
              "      <td>0.670105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.233100</td>\n",
              "      <td>0.519620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.212900</td>\n",
              "      <td>0.471136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.431450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.178200</td>\n",
              "      <td>0.400026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.159100</td>\n",
              "      <td>0.375649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.167300</td>\n",
              "      <td>0.362508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.140800</td>\n",
              "      <td>0.354640</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Models/TinyLlama-1B-MITRE/tokenizer_config.json',\n",
              " 'Models/TinyLlama-1B-MITRE/special_tokens_map.json',\n",
              " 'Models/TinyLlama-1B-MITRE/tokenizer.model',\n",
              " 'Models/TinyLlama-1B-MITRE/added_tokens.json',\n",
              " 'Models/TinyLlama-1B-MITRE/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "test_prompts, test_labels = [], []\n",
        "for user_content, assistant_content in iter_dataset(\"test.json\"):\n",
        "  test_prompts.append(build_prompt(user_content))\n",
        "  test_labels.append(assistant_content)\n",
        "\n",
        "llm = LLM(output_dir, dtype=\"float16\")\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "  max_tokens=10,\n",
        "  temperature=0.0,\n",
        ")\n",
        "\n",
        "outputs = llm.generate(test_prompts, sampling_params)\n",
        "\n",
        "results = []\n",
        "for out, label in zip(outputs, test_labels):\n",
        "  pred = out.outputs[0].text.strip()\n",
        "  results.append(pred == label)\n",
        "\n",
        "accuracy = sum(results) / len(results)\n",
        "print(f\"Test Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "tb0uwlHhAPUL",
        "outputId": "8e3f8cd9-2508-4139-b94d-8540b4c62c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vllm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-4165647904.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSamplingParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massistant_content\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtest_prompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vllm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}